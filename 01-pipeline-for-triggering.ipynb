{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# @title Copyright & License (click to expand)\n",
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsv4jGuU89rX"
   },
   "source": [
    "# Vertex AI Pipelines with KFP 2.x\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/kfp2_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/kfp2_pipeline.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "    <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/pipelines/kfp2_pipeline.ipynb\">\n",
    "       <img src=\"https://www.gstatic.com/cloud/images/navigation/vertex-ai.svg\" alt=\"Vertex AI logo\">Open in Vertex AI Workbench\n",
    "    </a>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:automl"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to use the Vertex AI Pipelines with KFP 2.x.\n",
    "\n",
    "Learn more about Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:pipelines,automl"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn to use `Vertex AI Pipelines` and KFP 2.x version of `Google Cloud Pipeline Components` to train and deploy an XGBoost model.\n",
    "\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Pipelines`\n",
    "- `Google Cloud Pipeline Components`\n",
    "- `BigQuery`\n",
    "\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a KFP pipeline:\n",
    "    - Create a `BigQuery Dataset` resource.\n",
    "    - Export the dataset.\n",
    "    - Train an XGBoost `Model` resource.\n",
    "    - Create an `Endpoint` resource.\n",
    "    - Deploys the `Model` resource to the `Endpoint` resource.\n",
    "- Compile the KFP pipeline.\n",
    "- Execute the KFP pipeline using `Vertex AI Pipelines`\n",
    "\n",
    "The components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/latest/google_cloud_pipeline_components.aiplatform.html#module-google_cloud_pipeline_components.aiplatform)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aef4f59195ad"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The [Census Income Data Set](https://archive.ics.uci.edu/ml/datasets/Census+Income) that this notebook uses for training is available publicly at the BigQuery location `bigquery-public-data.ml_datasets.census_adult_income`. It consists of the following fields:\n",
    "\n",
    "- `age`: Age.\n",
    "- `workclass`: Nature of employment.\n",
    "- `functional_weight`: Sample weight of the individual from the original Census data. How likely they were to be included in this dataset, based on their demographic characteristics vs. whole-population estimates.\n",
    "- `education`: Level of education completed.\n",
    "- `education_num`: Estimated years of education completed based on the value of the education field.\n",
    "- `marital_status`: Marital status.\n",
    "- `occupation`: Occupation category.\n",
    "- `relationship`: Relationship to the household.\n",
    "- `race`: Race.\n",
    "- `sex`: Gender.\n",
    "- `capital_gain`: Amount of capital gains.\n",
    "- `capital_loss`: Amount of capital loss.\n",
    "- `hours_per_week`: Hours worked per week.\n",
    "- `native_country`: Country of birth.\n",
    "- `income_bracket`: Either \" >50K\" or \" <=50K\" based on income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "costs"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [BigQuery pricing](https://cloud.google.com/bigquery/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9dJ5Of-dORl"
   },
   "outputs": [],
   "source": [
    "# ! pip3 install --no-cache-dir --upgrade \"kfp>2\" \\\n",
    "#                                         google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Colab only: Uncomment the following cell to restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-ZBOjErv5mM"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GPgNN7eeX1l"
   },
   "source": [
    "Check the KFP SDK version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NN0mULkEeb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.3.0\n",
      "google-cloud-aiplatform==1.34.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "before_you_begin:nogpu"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFIX = pipe-triggers-v1\n"
     ]
    }
   ],
   "source": [
    "# naming convention for all cloud resources\n",
    "VERSION        = \"v1\"                         # TODO - @ param {type:\"string\"}\n",
    "PREFIX         = f'pipe-triggers-{VERSION}'   # TODO - @ param {type:\"string\"}\n",
    "\n",
    "print(f\"PREFIX = {PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load env config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PROJECT_ID             = \"hybrid-vertex\"\n",
      "PROJECT_NUM            = \"934903580331\"\n",
      "\n",
      "REGION                 = \"us-central1\"\n",
      "BQ_REGION              = \"US\"\n",
      "BQ_PUBLIC_DS_URI       = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "DATASET_ID             = \"census_pipe_triggers_v1\"\n",
      "VIEW_NAME              = \"census_data\"\n",
      "BQ_LOG_DATA_URI        = \"hybrid-vertex.census_pipe_triggers_v1.census_training_table\"\n",
      "\n",
      "VPC_NETWORK_NAME       = \"ucaip-haystack-vpc-network\"\n",
      "VPC_NETWORK_FULL       = \"projects/934903580331/global/networks/ucaip-haystack-vpc-network\"\n",
      "\n",
      "SERVICE_ACCOUNT        = \"934903580331-compute@developer.gserviceaccount.com\"\n",
      "\n",
      "VERSION                = \"v1\"\n",
      "PREFIX                 = \"pipe-triggers-v1\"\n",
      "\n",
      "BUCKET_NAME            = \"pipe-triggers-v1-hybrid-vertex\"\n",
      "BUCKET_URI             = \"gs://pipe-triggers-v1-hybrid-vertex\"\n",
      "\n",
      "TOPIC_PATH             = \"projects/hybrid-vertex/topics/pipe-triggers-v1-topic\"\n",
      "PUBSUB_TOPIC           = \"projects/hybrid-vertex/topics/pipe-triggers-v1-topic\"\n",
      "\n",
      "PIPELINE_DISPLAY_NAME  = \"census-pipe-triggers-v1\"\n",
      "PIPELINE_ROOT_PATH     = \"gs://pipe-triggers-v1-hybrid-vertex/census_pipeline_root\"\n",
      "PIPELINE_YAML_FILENAME = \"pipeline.yaml\"\n",
      "PIPELINES_FILEPATH     = \"gs://pipe-triggers-v1-hybrid-vertex/census_pipeline_root/pipeline-spec/pipeline.yaml\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# staging GCS\n",
    "GCP_PROJECTS             = !gcloud config get-value project\n",
    "PROJECT_ID               = GCP_PROJECTS[0]\n",
    "\n",
    "# GCS bucket and paths\n",
    "BUCKET_NAME              = f'{PREFIX}-{PROJECT_ID}'\n",
    "BUCKET_URI               = f'gs://{BUCKET_NAME}'\n",
    "\n",
    "config = !gsutil cat {BUCKET_URI}/config/notebook_env.py\n",
    "print(config.n)\n",
    "exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gcp IAM & Networking\n",
    "# SERVICE_ACCOUNT  = \"[your-service-account]\"     # TODO - leave blank to use compute-engine default SA\n",
    "# VPC_NETWORK_NAME = \"ucaip-haystack-vpc-network\" # TODO - @ param {type:\"string\"}\n",
    "\n",
    "# # bigquery\n",
    "# DATASET_ID       = \"census\"  # The Data Set ID where the view sits\n",
    "# VIEW_NAME        = \"census_data\"  # BigQuery view you create for input data\n",
    "\n",
    "# REGION           = \"us-central1\"\n",
    "# BQ_REGION        = REGION.split(\"-\")[0].upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2dw8q9fdQEH5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGION    = us-central1\n",
      "BQ_REGION = US\n"
     ]
    }
   ],
   "source": [
    "print(f\"REGION    = {REGION}\")\n",
    "print(f\"BQ_REGION = {BQ_REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcp_authenticate"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below.\n",
    "\n",
    "**1. Vertex AI Workbench**\n",
    "* Do nothing as you are already authenticated.\n",
    "\n",
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ce6043da7b33"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0367eac06a10"
   },
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "21ad4dbb4a61"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c13224697bfb"
   },
   "source": [
    "**4. Service account or other**\n",
    "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "# BUCKET_URI = f\"gs://{PREFIX}-{PROJECT_ID}\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "autoset_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "91c46850b49b"
   },
   "outputs": [],
   "source": [
    "# ! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85c4ecfd133a"
   },
   "source": [
    "#### Service Account \n",
    "\n",
    "You use a service account to create Vertex AI Pipeline jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "f936bebda2d4"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# IS_COLAB = \"google.colab\" in sys.modules\n",
    "# if (\n",
    "#     SERVICE_ACCOUNT == \"\"\n",
    "#     or SERVICE_ACCOUNT is None\n",
    "#     or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "# ):\n",
    "#     # Get your service account from gcloud\n",
    "#     if not IS_COLAB:\n",
    "#         shell_output = !gcloud auth list 2>/dev/null\n",
    "#         SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "#     else:  # IS_COLAB:\n",
    "#         shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "#         project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "#         SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "#     print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40ef6967cad3"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step. You only need to run this step once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f88cb0488c08"
   },
   "outputs": [],
   "source": [
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "# ! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "V0aexAES_cnZ"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "## Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VB_XJHA3iccD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT_PATH: gs://pipe-triggers-v1-hybrid-vertex/census_pipeline_root\n"
     ]
    }
   ],
   "source": [
    "# PATH = %env PATH\n",
    "# %env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "# KFP_ENDPOINT = (\n",
    "#     \"https://720c5bc00c3d6089-dot-us-central1.pipelines.googleusercontent.com/\"\n",
    "# )\n",
    "\n",
    "# PIPELINE_ROOT = f\"{BUCKET_URI}/census_pipeline\"\n",
    "print(f\"PIPELINE_ROOT_PATH: {PIPELINE_ROOT_PATH}\") # This is where all pipeline artifacts are sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "052ca6eeecaf"
   },
   "source": [
    "### Create a BigQuery dataset\n",
    "\n",
    "Next, you create a BQ dataset for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wzehWtGpBIes"
   },
   "outputs": [],
   "source": [
    "# Create a BQ Dataset in the project.\n",
    "# !bq mk --location=$BQ_REGION --dataset $PROJECT_ID:$DATASET_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce90503cfe4"
   },
   "source": [
    "## Define components for the pipeline\n",
    "\n",
    "Next, you define several components that you use in your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65183ad63691"
   },
   "source": [
    "### Define create BigQuery dataset component\n",
    "\n",
    "First, you define a component to create BigQuery dataset view from the public dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ac414f17"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery==3.10.0\"],\n",
    ")\n",
    "def create_census_view(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    view_name: str,\n",
    "):\n",
    "    \"\"\"Creates a BigQuery view on `bigquery-public-data.ml_datasets.census_adult_income`.\n",
    "\n",
    "    Args:\n",
    "        project_id: The Project ID.\n",
    "        dataset_id: The BigQuery Dataset ID. Must be pre-created in the project.\n",
    "        view_name: The BigQuery view name.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    create_or_replace_view = \"\"\"\n",
    "        CREATE OR REPLACE VIEW\n",
    "        `{dataset_id}`.`{view_name}` AS\n",
    "        SELECT\n",
    "          age,\n",
    "          workclass,\n",
    "          education,\n",
    "          education_num,\n",
    "          marital_status,\n",
    "          occupation,\n",
    "          relationship,\n",
    "          race,\n",
    "          sex,\n",
    "          capital_gain,\n",
    "          capital_loss,\n",
    "          hours_per_week,\n",
    "          native_country,\n",
    "          income_bracket,\n",
    "        FROM\n",
    "          `bigquery-public-data.ml_datasets.census_adult_income`\n",
    "    \"\"\".format(\n",
    "        dataset_id=dataset_id, view_name=view_name\n",
    "    )\n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query=create_or_replace_view, job_config=job_config)\n",
    "    query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94fb3fd6e079"
   },
   "source": [
    "### Define export dataset component\n",
    "\n",
    "Next, you define a component to export the data from your BigQuery dataset to use for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "v9NXQwPlFMgW"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery[pandas]==3.10.0\"],\n",
    ")\n",
    "def export_dataset(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    view_name: str,\n",
    "    dataset: Output[Dataset],\n",
    "):\n",
    "    \"\"\"Exports from BigQuery to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        project_id: The Project ID.\n",
    "        dataset_id: The BigQuery Dataset ID. Must be pre-created in the project.\n",
    "        view_name: The BigQuery view name.\n",
    "\n",
    "    Returns:\n",
    "        dataset: The Dataset artifact with exported CSV file.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    table_name = f\"{project_id}.{dataset_id}.{view_name}\"\n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      `{table_name}`\n",
    "    \"\"\".format(\n",
    "        table_name=table_name\n",
    "    )\n",
    "\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query=query, job_config=job_config)\n",
    "    df = query_job.result().to_dataframe()\n",
    "    df.to_csv(dataset.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8078c3ccf485"
   },
   "source": [
    "### Define XGBoost training component\n",
    "\n",
    "Next, you define a component to train an XGBoost model with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "211c652f"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"xgboost==1.6.2\",\n",
    "        \"pandas==1.3.5\",\n",
    "        \"joblib==1.1.0\",\n",
    "        \"scikit-learn==1.0.2\",\n",
    "    ],\n",
    ")\n",
    "def xgboost_training(\n",
    "    dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "):\n",
    "    \"\"\"Trains an XGBoost classifier.\n",
    "\n",
    "    Args:\n",
    "        dataset: The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        model: The model artifact stores the model.joblib file.\n",
    "        metrics: The metrics of the trained model.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import (accuracy_score, precision_recall_curve,\n",
    "                                 roc_auc_score)\n",
    "    from sklearn.model_selection import (RandomizedSearchCV, StratifiedKFold,\n",
    "                                         train_test_split)\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    # Load the training census dataset\n",
    "    with open(dataset.path, \"r\") as train_data:\n",
    "        raw_data = pd.read_csv(train_data)\n",
    "\n",
    "    CATEGORICAL_COLUMNS = (\n",
    "        \"workclass\",\n",
    "        \"education\",\n",
    "        \"marital_status\",\n",
    "        \"occupation\",\n",
    "        \"relationship\",\n",
    "        \"race\",\n",
    "        \"sex\",\n",
    "        \"native_country\",\n",
    "    )\n",
    "    LABEL_COLUMN = \"income_bracket\"\n",
    "    POSITIVE_VALUE = \" >50K\"\n",
    "\n",
    "    # Convert data in categorical columns to numerical values\n",
    "    encoders = {col: LabelEncoder() for col in CATEGORICAL_COLUMNS}\n",
    "    for col in CATEGORICAL_COLUMNS:\n",
    "        raw_data[col] = encoders[col].fit_transform(raw_data[col])\n",
    "\n",
    "    X = raw_data.drop([LABEL_COLUMN], axis=1).values\n",
    "    y = raw_data[LABEL_COLUMN] == POSITIVE_VALUE\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    _ = xgb.DMatrix(X_train, label=y_train)\n",
    "    _ = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "        \"reg_lambda\": [0, 1],\n",
    "        \"gamma\": [1, 1.5, 2, 2.5, 3],\n",
    "        \"max_depth\": [2, 3, 4, 5, 10, 20],\n",
    "        \"learning_rate\": [0.1, 0.01],\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=50,\n",
    "        objective=\"binary:hinge\",\n",
    "        silent=True,\n",
    "        nthread=1,\n",
    "        eval_metric=\"auc\",\n",
    "    )\n",
    "\n",
    "    folds = 5\n",
    "    param_comb = 20\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_distributions=params,\n",
    "        n_iter=param_comb,\n",
    "        scoring=\"precision\",\n",
    "        n_jobs=4,\n",
    "        cv=skf.split(X_train, y_train),\n",
    "        verbose=4,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "    xgb_model_best = random_search.best_estimator_\n",
    "    predictions = xgb_model_best.predict(X_test)\n",
    "    score = accuracy_score(y_test, predictions)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    _ = precision_recall_curve(y_test, predictions)\n",
    "\n",
    "    metrics.log_metric(\"accuracy\", (score * 100.0))\n",
    "    metrics.log_metric(\"framework\", \"xgboost\")\n",
    "    metrics.log_metric(\"dataset_size\", len(raw_data))\n",
    "    metrics.log_metric(\"AUC\", auc)\n",
    "\n",
    "    # Export the model to a file\n",
    "    os.makedirs(model.path, exist_ok=True)\n",
    "    joblib.dump(xgb_model_best, os.path.join(model.path, \"model.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "242c3348d54d"
   },
   "source": [
    "### Define deploying the model component\n",
    "\n",
    "Finally, you define a component to deploy the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wMSORMSQvlwO"
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform==1.34.0\"],\n",
    ")\n",
    "def deploy_xgboost_model(\n",
    "    model: Input[Model],\n",
    "    project_id: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model],\n",
    "):\n",
    "    \"\"\"Deploys an XGBoost model to Vertex AI Endpoint.\n",
    "\n",
    "    Args:\n",
    "        model: The model to deploy.\n",
    "        project_id: The project ID of the Vertex AI Endpoint.\n",
    "\n",
    "    Returns:\n",
    "        vertex_endpoint: The deployed Vertex AI Endpoint.\n",
    "        vertex_model: The deployed Vertex AI Model.\n",
    "    \"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project_id)\n",
    "\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"census-demo-model\",\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-6:latest\",\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb7d14f7d3db"
   },
   "source": [
    "## Construct the XGBoost training pipeline\n",
    "\n",
    "Now you define the pipeline, with the following steps:\n",
    "\n",
    "- Create a BigQuery view of the dataset.\n",
    "- Export the dataset.\n",
    "- Train the model.\n",
    "- Deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "df55e79c"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"census-demo-pipeline\",\n",
    ")\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    view_name: str,\n",
    "):\n",
    "    \"\"\"A demo pipeline.\"\"\"\n",
    "\n",
    "    create_input_view_task = create_census_view(\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        view_name=view_name,\n",
    "    )\n",
    "\n",
    "    export_dataset_task = (\n",
    "        export_dataset(\n",
    "            project_id=project_id,\n",
    "            dataset_id=dataset_id,\n",
    "            view_name=view_name,\n",
    "        )\n",
    "        .after(create_input_view_task)\n",
    "        .set_caching_options(False)\n",
    "    )\n",
    "\n",
    "    training_task = xgboost_training(\n",
    "        dataset=export_dataset_task.outputs[\"dataset\"],\n",
    "    )\n",
    "\n",
    "    _ = deploy_xgboost_model(\n",
    "        project_id=project_id,\n",
    "        model=training_task.outputs[\"model\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_train_pipeline:icn"
   },
   "source": [
    "### Compile the pipeline\n",
    "\n",
    "Next, you compile the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_URI             : gs://pipe-triggers-v1-hybrid-vertex\n",
      "PIPELINE_ROOT_PATH     : gs://pipe-triggers-v1-hybrid-vertex/census_pipeline_root\n",
      "PIPELINE_YAML_FILENAME : pipeline.yaml\n",
      "PIPELINES_FILEPATH     : gs://pipe-triggers-v1-hybrid-vertex/census_pipeline_root/pipeline-spec/pipeline.yaml\n"
     ]
    }
   ],
   "source": [
    "print(f\"BUCKET_URI             : {BUCKET_URI}\")\n",
    "print(f\"PIPELINE_ROOT_PATH     : {PIPELINE_ROOT_PATH}\")\n",
    "print(f\"PIPELINE_YAML_FILENAME : {PIPELINE_YAML_FILENAME}\")\n",
    "print(f\"PIPELINES_FILEPATH     : {PIPELINES_FILEPATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4abfd490"
   },
   "outputs": [],
   "source": [
    "! rm -f $PIPELINE_YAML_FILENAME\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, \n",
    "    package_path=PIPELINE_YAML_FILENAME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy pipeline definition to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://pipeline.yaml [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 17.0 KiB/ 17.0 KiB]                                                \n",
      "Operation completed over 1 objects/17.0 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp $PIPELINE_YAML_FILENAME $PIPELINES_FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://pipe-triggers-v1-hybrid-vertex/census_pipeline_root/pipeline.yaml\n",
      "gs://pipe-triggers-v1-hybrid-vertex/census_pipeline_root/pipeline-spec/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls $PIPELINE_ROOT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lz99NXISmL0I"
   },
   "source": [
    "### Run the pipeline using Vertex AI Pipelines\n",
    "\n",
    "Now, run the compiled pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "c2cd77f23d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/census-demo-pipeline-20231012145759\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/934903580331/locations/us-central1/pipelineJobs/census-demo-pipeline-20231012145759')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/census-demo-pipeline-20231012145759?project=934903580331\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/census-demo-pipeline-20231012145759 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/census-demo-pipeline-20231012145759 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/census-demo-pipeline-20231012145759 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/census-demo-pipeline-20231012145759 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/census-demo-pipeline-20231012145759 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/census-demo-pipeline-20231012145759 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/934903580331/locations/us-central1/pipelineJobs/census-demo-pipeline-20231012145759 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/934903580331/locations/us-central1/pipelineJobs/census-demo-pipeline-20231012145759\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=PIPELINE_DISPLAY_NAME,\n",
    "    template_path=PIPELINES_FILEPATH,\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    "    enable_caching=False,\n",
    "    failure_policy='fast', # slow | fast\n",
    "    parameter_values={\n",
    "        'project_id':PROJECT_ID,\n",
    "        'view_name':VIEW_NAME,\n",
    "        'dataset_id':DATASET_ID\n",
    "    }   \n",
    ")\n",
    "\n",
    "job.run(\n",
    "    sync=False,\n",
    "    service_account=SERVICE_ACCOUNT, # SERVICE_ACCOUNT\n",
    "    # network=f'projects/{PROJECT_NUM}/global/networks/{VPC_NETWORK_NAME}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38c373adbe83"
   },
   "source": [
    "### Run the pipeline using KFP\n",
    "\n",
    "Alternatively, you can run the pipeline using KFP directly.\n",
    "\n",
    "*Note:* You need to provider their own value for `KFP_ENDPOINT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdpgePHJl4VJ"
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     KFP_ENDPOINT = \"[your-kfp-endpoint]\"\n",
    "\n",
    "#     client = kfp.Client(\n",
    "#         host=KFP_ENDPOINT,\n",
    "#     )\n",
    "\n",
    "#     client.create_run_from_pipeline_package(\n",
    "#         pipeline_file=\"pipeline.yaml\",\n",
    "#         pipeline_root=PIPELINE_ROOT,\n",
    "#         run_name=\"census-demo-pipeline\",\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24c8f4c2aec4"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daa9b68da39f"
   },
   "outputs": [],
   "source": [
    "# delete_bucket = True\n",
    "\n",
    "# job.delete()\n",
    "\n",
    "# if delete_bucket or os.getenv(\"ID_TESTING\"):\n",
    "#     ! gsutil rm -rf {BUCKET_URI}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "kfp2_pipeline.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m106",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m106"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
